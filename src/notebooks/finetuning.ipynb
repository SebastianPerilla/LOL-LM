{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 for Satirical News Generation\n",
    "\n",
    "This notebook fine-tunes GPT-2 to generate satirical news articles based on topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-15 11:54:33\u001b[0m | \u001b[34m\u001b[1mDEBUG\u001b[0m | Loaded environment variables from: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM\n",
      "Data dir: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/fake_news/processed\n",
      "Model dir: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Fine-tuning notebook for GPT-2 on satirical news generation.\n",
    "\n",
    "This notebook:\n",
    "1. Loads the processed satirical news dataset\n",
    "2. Extracts keywords from headlines and articles using NLP\n",
    "3. Formats data for fine-tuning with system/user prompts\n",
    "4. Fine-tunes GPT-2 model\n",
    "5. Saves checkpoints during training\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Determine project root\n",
    "if Path.cwd().name == \"LOL-LM\":\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif (Path.cwd() / \"src\" / \"notebooks\").exists():\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif (Path.cwd().parent / \"src\" / \"notebooks\").exists():\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    PROJECT_ROOT = Path(__file__).parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent\n",
    "\n",
    "# Add src to path for imports\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from utils import env  # noqa: F401 - loads .env file\n",
    "from logger import log\n",
    "\n",
    "# Set up paths\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"fake_news\" / \"processed\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"data\" / \"model\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Model dir: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "install",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "try:\n",
    "    import spacy\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "    from datasets import Dataset\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"datasets\", \"accelerate\", \"spacy\"])\n",
    "    import spacy\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Download spaCy model\n",
    "    print(\"Downloading spaCy English model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "print(\"‚úÖ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tune dataset: 10793 rows from /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/fake_news/processed/babylonbee_finetune.csv\n",
      "Columns: ['Headline', 'Article', 'topics', 'system', 'user', 'result', 'training_text']\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>topics</th>\n",
       "      <th>system</th>\n",
       "      <th>user</th>\n",
       "      <th>result</th>\n",
       "      <th>training_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brave Adventurer Discovers Long-Lost Article H...</td>\n",
       "      <td>MOAB, UT ‚Äî A historic discovery was made today...</td>\n",
       "      <td>Brave Adventurer, Long-Lost Article, Hidden Be...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Brave Adventurer, Long...</td>\n",
       "      <td>Headline: Brave Adventurer Discovers Long-Lost...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Irishmen Say They Understood Biden's Dub...</td>\n",
       "      <td>DUBLIN ‚Äî Despite claims from conservative medi...</td>\n",
       "      <td>Drunk Irishmen, Biden's Dublin Speech, Dublin ...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Drunk Irishmen, Biden'...</td>\n",
       "      <td>Headline: Drunk Irishmen Say They Understood B...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Leguizamo's Boycott Of Mario Movie Leads ...</td>\n",
       "      <td>MANHATTAN, NY ‚Äî With The Super Mario Bros. Mov...</td>\n",
       "      <td>John Leguizamo's Boycott, Mario Movie, Sharp Rise</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: John Leguizamo's Boyco...</td>\n",
       "      <td>Headline: John Leguizamo's Boycott Of Mario Mo...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pentagon Leaker Kicking Himself For Not Just L...</td>\n",
       "      <td>DIGHTON, MA ‚Äî Military police have arrested Ja...</td>\n",
       "      <td>Classified Documents, His Garage, Pentagon Lea...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Classified Documents, ...</td>\n",
       "      <td>Headline: Pentagon Leaker Kicking Himself For ...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parents Just Relieved Teen Who Came Home Drunk...</td>\n",
       "      <td>NEW BRITAIN, PA ‚Äî Local parents Tim and Julia ...</td>\n",
       "      <td>Just Relieved Teen, Bud Light</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Just Relieved Teen, Bu...</td>\n",
       "      <td>Headline: Parents Just Relieved Teen Who Came ...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Brave Adventurer Discovers Long-Lost Article H...   \n",
       "1  Drunk Irishmen Say They Understood Biden's Dub...   \n",
       "2  John Leguizamo's Boycott Of Mario Movie Leads ...   \n",
       "3  Pentagon Leaker Kicking Himself For Not Just L...   \n",
       "4  Parents Just Relieved Teen Who Came Home Drunk...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  MOAB, UT ‚Äî A historic discovery was made today...   \n",
       "1  DUBLIN ‚Äî Despite claims from conservative medi...   \n",
       "2  MANHATTAN, NY ‚Äî With The Super Mario Bros. Mov...   \n",
       "3  DIGHTON, MA ‚Äî Military police have arrested Ja...   \n",
       "4  NEW BRITAIN, PA ‚Äî Local parents Tim and Julia ...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  Brave Adventurer, Long-Lost Article, Hidden Be...   \n",
       "1  Drunk Irishmen, Biden's Dublin Speech, Dublin ...   \n",
       "2  John Leguizamo's Boycott, Mario Movie, Sharp Rise   \n",
       "3  Classified Documents, His Garage, Pentagon Lea...   \n",
       "4                      Just Relieved Teen, Bud Light   \n",
       "\n",
       "                                              system  \\\n",
       "0  You are a satirical news generator. When given...   \n",
       "1  You are a satirical news generator. When given...   \n",
       "2  You are a satirical news generator. When given...   \n",
       "3  You are a satirical news generator. When given...   \n",
       "4  You are a satirical news generator. When given...   \n",
       "\n",
       "                                                user  \\\n",
       "0  Generate an article on: Brave Adventurer, Long...   \n",
       "1  Generate an article on: Drunk Irishmen, Biden'...   \n",
       "2  Generate an article on: John Leguizamo's Boyco...   \n",
       "3  Generate an article on: Classified Documents, ...   \n",
       "4  Generate an article on: Just Relieved Teen, Bu...   \n",
       "\n",
       "                                              result  \\\n",
       "0  Headline: Brave Adventurer Discovers Long-Lost...   \n",
       "1  Headline: Drunk Irishmen Say They Understood B...   \n",
       "2  Headline: John Leguizamo's Boycott Of Mario Mo...   \n",
       "3  Headline: Pentagon Leaker Kicking Himself For ...   \n",
       "4  Headline: Parents Just Relieved Teen Who Came ...   \n",
       "\n",
       "                                       training_text  \n",
       "0  <system>You are a satirical news generator. Wh...  \n",
       "1  <system>You are a satirical news generator. Wh...  \n",
       "2  <system>You are a satirical news generator. Wh...  \n",
       "3  <system>You are a satirical news generator. Wh...  \n",
       "4  <system>You are a satirical news generator. Wh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the processed dataset or cached fine-tuning file\n",
    "raw_dataset_path = DATA_DIR / \"babylonbee_processed.csv\"\n",
    "finetune_path = DATA_DIR / \"babylonbee_finetune.csv\"\n",
    "\n",
    "finetune_exists = finetune_path.exists()\n",
    "\n",
    "if finetune_exists:\n",
    "    df = pd.read_csv(finetune_path)\n",
    "    print(f\"Loaded fine-tune dataset: {len(df)} rows from {finetune_path}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "else:\n",
    "    if not raw_dataset_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {raw_dataset_path}. Please run prep_sarcasm.ipynb first.\")\n",
    "    df = pd.read_csv(raw_dataset_path)\n",
    "    print(f\"Loaded base dataset: {len(df)} rows from {raw_dataset_path}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load_spacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded spaCy English model\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for NLP keyword extraction\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Loaded spaCy English model\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è spaCy model not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"‚úÖ Loaded spaCy English model\")\n",
    "\n",
    "# Define stopwords (spaCy's stopwords + common irrelevant words)\n",
    "stopwords = set[str](nlp.Defaults.stop_words)\n",
    "stopwords.update(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just', 'now'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extract_keywords",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted topics sample: Mom, son's web series closest thing\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def _phrases_from_doc(doc):\n",
    "    phrases = [chunk.text.strip() for chunk in doc.noun_chunks if len(chunk.text.strip()) > 2]\n",
    "    ents = [ent.text.strip() for ent in doc.ents if len(ent.text.strip()) > 2]\n",
    "    return phrases + ents\n",
    "\n",
    "\n",
    "def extract_topics(headline: str, article: str, max_keywords: int = 3) -> str:\n",
    "    \"\"\"Headline-first topic extraction using noun chunks + entities.\n",
    "    Falls back to the first article sentence when headline yields nothing.\"\"\"\n",
    "    candidates: List[str] = []\n",
    "    headline = (headline or \"\").strip()\n",
    "    article = (article or \"\").strip()\n",
    "\n",
    "    if headline:\n",
    "        doc = nlp(headline)\n",
    "        candidates = _phrases_from_doc(doc)\n",
    "\n",
    "    if not candidates and article:\n",
    "        # Use first sentence of article to keep cost low\n",
    "        first_sentence = next(nlp(article).sents, None)\n",
    "        if first_sentence:\n",
    "            candidates = _phrases_from_doc(first_sentence)\n",
    "\n",
    "    filtered = []\n",
    "    seen = set()\n",
    "    for cand in candidates:\n",
    "        norm = cand.lower()\n",
    "        if norm in seen:\n",
    "            continue\n",
    "        tokens = re.split(r\"\\s+\", cand)\n",
    "        if all(tok.lower() in stopwords for tok in tokens):\n",
    "            continue\n",
    "        if len(cand) < 3:\n",
    "            continue\n",
    "        seen.add(norm)\n",
    "        filtered.append(cand)\n",
    "        if len(filtered) >= max_keywords:\n",
    "            break\n",
    "\n",
    "    if not filtered:\n",
    "        return \"news, article\"\n",
    "    return \", \".join(filtered)\n",
    "\n",
    "# Quick check\n",
    "_test_headline = \"Mom starting to fear son's web series closest thing she'll have to grandchild\"\n",
    "print(\"Extracted topics sample:\", extract_topics(_test_headline, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apply_keywords",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping keyword extraction; using existing file at /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/fake_news/processed/babylonbee_finetune.csv\n",
      "Top 5 rows from cached fine-tune dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>topics</th>\n",
       "      <th>system</th>\n",
       "      <th>user</th>\n",
       "      <th>result</th>\n",
       "      <th>training_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brave Adventurer Discovers Long-Lost Article H...</td>\n",
       "      <td>MOAB, UT ‚Äî A historic discovery was made today...</td>\n",
       "      <td>Brave Adventurer, Long-Lost Article, Hidden Be...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Brave Adventurer, Long...</td>\n",
       "      <td>Headline: Brave Adventurer Discovers Long-Lost...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Irishmen Say They Understood Biden's Dub...</td>\n",
       "      <td>DUBLIN ‚Äî Despite claims from conservative medi...</td>\n",
       "      <td>Drunk Irishmen, Biden's Dublin Speech, Dublin ...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Drunk Irishmen, Biden'...</td>\n",
       "      <td>Headline: Drunk Irishmen Say They Understood B...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Leguizamo's Boycott Of Mario Movie Leads ...</td>\n",
       "      <td>MANHATTAN, NY ‚Äî With The Super Mario Bros. Mov...</td>\n",
       "      <td>John Leguizamo's Boycott, Mario Movie, Sharp Rise</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: John Leguizamo's Boyco...</td>\n",
       "      <td>Headline: John Leguizamo's Boycott Of Mario Mo...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pentagon Leaker Kicking Himself For Not Just L...</td>\n",
       "      <td>DIGHTON, MA ‚Äî Military police have arrested Ja...</td>\n",
       "      <td>Classified Documents, His Garage, Pentagon Lea...</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Classified Documents, ...</td>\n",
       "      <td>Headline: Pentagon Leaker Kicking Himself For ...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parents Just Relieved Teen Who Came Home Drunk...</td>\n",
       "      <td>NEW BRITAIN, PA ‚Äî Local parents Tim and Julia ...</td>\n",
       "      <td>Just Relieved Teen, Bud Light</td>\n",
       "      <td>You are a satirical news generator. When given...</td>\n",
       "      <td>Generate an article on: Just Relieved Teen, Bu...</td>\n",
       "      <td>Headline: Parents Just Relieved Teen Who Came ...</td>\n",
       "      <td>&lt;system&gt;You are a satirical news generator. Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Brave Adventurer Discovers Long-Lost Article H...   \n",
       "1  Drunk Irishmen Say They Understood Biden's Dub...   \n",
       "2  John Leguizamo's Boycott Of Mario Movie Leads ...   \n",
       "3  Pentagon Leaker Kicking Himself For Not Just L...   \n",
       "4  Parents Just Relieved Teen Who Came Home Drunk...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  MOAB, UT ‚Äî A historic discovery was made today...   \n",
       "1  DUBLIN ‚Äî Despite claims from conservative medi...   \n",
       "2  MANHATTAN, NY ‚Äî With The Super Mario Bros. Mov...   \n",
       "3  DIGHTON, MA ‚Äî Military police have arrested Ja...   \n",
       "4  NEW BRITAIN, PA ‚Äî Local parents Tim and Julia ...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  Brave Adventurer, Long-Lost Article, Hidden Be...   \n",
       "1  Drunk Irishmen, Biden's Dublin Speech, Dublin ...   \n",
       "2  John Leguizamo's Boycott, Mario Movie, Sharp Rise   \n",
       "3  Classified Documents, His Garage, Pentagon Lea...   \n",
       "4                      Just Relieved Teen, Bud Light   \n",
       "\n",
       "                                              system  \\\n",
       "0  You are a satirical news generator. When given...   \n",
       "1  You are a satirical news generator. When given...   \n",
       "2  You are a satirical news generator. When given...   \n",
       "3  You are a satirical news generator. When given...   \n",
       "4  You are a satirical news generator. When given...   \n",
       "\n",
       "                                                user  \\\n",
       "0  Generate an article on: Brave Adventurer, Long...   \n",
       "1  Generate an article on: Drunk Irishmen, Biden'...   \n",
       "2  Generate an article on: John Leguizamo's Boyco...   \n",
       "3  Generate an article on: Classified Documents, ...   \n",
       "4  Generate an article on: Just Relieved Teen, Bu...   \n",
       "\n",
       "                                              result  \\\n",
       "0  Headline: Brave Adventurer Discovers Long-Lost...   \n",
       "1  Headline: Drunk Irishmen Say They Understood B...   \n",
       "2  Headline: John Leguizamo's Boycott Of Mario Mo...   \n",
       "3  Headline: Pentagon Leaker Kicking Himself For ...   \n",
       "4  Headline: Parents Just Relieved Teen Who Came ...   \n",
       "\n",
       "                                       training_text  \n",
       "0  <system>You are a satirical news generator. Wh...  \n",
       "1  <system>You are a satirical news generator. Wh...  \n",
       "2  <system>You are a satirical news generator. Wh...  \n",
       "3  <system>You are a satirical news generator. Wh...  \n",
       "4  <system>You are a satirical news generator. Wh...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract keywords from all headlines and articles (only if not already processed)\n",
    "if finetune_exists:\n",
    "    print(f\"Skipping keyword extraction; using existing file at {finetune_path}\")\n",
    "    print(\"Top 5 rows from cached fine-tune dataset:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Extracting topics from dataset (headline-first noun chunks/entities)...\")\n",
    "\n",
    "    def extract_topics_from_row(row):\n",
    "        headline = str(row.get('Headline', ''))\n",
    "        article = str(row.get('Article', ''))\n",
    "        return extract_topics(headline, article, max_keywords=3)\n",
    "\n",
    "    tqdm.pandas(desc=\"Extracting topics\")\n",
    "    df['topics'] = df.progress_apply(extract_topics_from_row, axis=1)\n",
    "\n",
    "    print(f\"\\n‚úÖ Extracted topics for {len(df)} rows\")\n",
    "    print(\"\\nSample topics:\")\n",
    "    print(df[['Headline', 'topics']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "format_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting training examples...\n",
      "Using existing fine-tune dataset; ensuring training_text is present\n",
      "‚úÖ Formatted 10793 training examples\n",
      "\n",
      "Sample training text:\n",
      "<system>You are a satirical news generator. When given a topic, generate a funny headline followed by the article.<user>Generate an article on: Brave Adventurer, Long-Lost Article, Hidden Beneath Labyrinth<assistant>Headline: Brave Adventurer Discovers Long-Lost Article Hidden Beneath Labyrinth Of Ads, Pop-Ups, Privacy Policies\n",
      "Article: MOAB, UT ‚Äî A historic discovery was made today as a brave adventurer uncovered an internet article long thought to be lost forever underneath layers upon layers ...\n"
     ]
    }
   ],
   "source": [
    "# Format data for fine-tuning\n",
    "# System prompt: \"You are a satirical news generator. When given a topic, generate a funny headline followed by the article.\"\n",
    "# User prompt: \"Generate an article on: {topics}\"\n",
    "# Generation format: \"Headline: {headline}\\nArticle: {article}\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a satirical news generator. When given a topic, generate a funny headline followed by the article.\"\n",
    "\n",
    "def format_training_example(row):\n",
    "    \"\"\"Format a single training example.\"\"\"\n",
    "    topics = row['topics']\n",
    "    headline = str(row['Headline'])\n",
    "    article = str(row['Article'])\n",
    "    \n",
    "    # Format the conversation\n",
    "    user_message = f\"Generate an article on: {topics}\"\n",
    "    assistant_message = f\"Headline: {headline}\\nArticle: {article}\"\n",
    "    \n",
    "    # Format for GPT-2 training (simple text format)\n",
    "    # We'll use a format like: <system>...<user>...<assistant>...\n",
    "    formatted_text = f\"<system>{SYSTEM_PROMPT}<user>{user_message}<assistant>{assistant_message}<|endoftext|>\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "print(\"Formatting training examples...\")\n",
    "\n",
    "if finetune_exists:\n",
    "    print(\"Using existing fine-tune dataset; ensuring training_text is present\")\n",
    "    required_columns = {'Headline', 'Article', 'topics', 'system', 'user', 'result'}\n",
    "    missing_columns = required_columns - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Existing fine-tune file is missing columns: {missing_columns}\")\n",
    "    if 'training_text' not in df.columns:\n",
    "        df['training_text'] = df.apply(format_training_example, axis=1)\n",
    "else:\n",
    "    df['system'] = SYSTEM_PROMPT\n",
    "    df['user'] = df['topics'].apply(lambda topics: f\"Generate an article on: {topics}\")\n",
    "    df['result'] = df.apply(lambda row: f\"Headline: {row['Headline']}\\nArticle: {row['Article']}\", axis=1)\n",
    "    df['training_text'] = df.apply(format_training_example, axis=1)\n",
    "\n",
    "    df.to_csv(finetune_path, index=False)\n",
    "    print(f\"‚úÖ Saved fine-tune dataset to {finetune_path}\")\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(df)} training examples\")\n",
    "print(\"\\nSample training text:\")\n",
    "print(df['training_text'].iloc[0][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2 model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "‚úÖ Loaded model with 124,443,648 parameters\n",
      "Vocabulary size: 50262\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "print(f\"Loading {model_name} model and tokenizer...\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add special tokens for our format\n",
    "special_tokens = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'eos_token': '<|endoftext|>',\n",
    "    'additional_special_tokens': ['<system>', '<user>', '<assistant>']\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Loaded model with {model.num_parameters():,} parameters\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prepare_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae8a727137c4f88a019ecd435601d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 9713\n",
      "‚úÖ Validation samples: 1080\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_texts = df['training_text'].tolist()\n",
    "dataset = Dataset.from_dict({'text': train_texts})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=512,  # GPT-2 context window\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Split into train/validation (90/10)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "training_args",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training arguments configured\n",
      "   Output directory: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/model/gpt2-satirical-news\n",
      "   Checkpoints will be saved every 500 steps\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "output_dir = MODEL_DIR / \"gpt2-satirical-news\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import inspect\n",
    "\n",
    "training_kwargs = dict(\n",
    "    output_dir=str(output_dir),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=device.type == \"cuda\",  # Mixed precision only on CUDA\n",
    "    logging_steps=50,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,  # Save checkpoint every 500 steps\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=5,  # Keep only last 5 checkpoints\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    dataloader_pin_memory=device.type == \"cuda\",\n",
    ")\n",
    "\n",
    "# Handle older transformers naming differences\n",
    "if \"evaluation_strategy\" not in inspect.signature(TrainingArguments).parameters and \"eval_strategy\" in inspect.signature(TrainingArguments).parameters:\n",
    "    training_kwargs[\"eval_strategy\"] = training_kwargs.pop(\"evaluation_strategy\")\n",
    "\n",
    "# Filter out kwargs that are not supported by the installed transformers version\n",
    "supported = set(inspect.signature(TrainingArguments).parameters.keys())\n",
    "filtered_kwargs = {k: v for k, v in training_kwargs.items() if k in supported}\n",
    "missing = set(training_kwargs.keys()) - supported\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è Skipping unsupported TrainingArguments keys for this transformers version: {missing}\")\n",
    "\n",
    "# Ensure eval/save strategies align when load_best_model_at_end is requested\n",
    "if filtered_kwargs.get(\"load_best_model_at_end\"):\n",
    "    save_strategy = filtered_kwargs.get(\"save_strategy\") or filtered_kwargs.get(\"save_strategy\", \"steps\")\n",
    "    eval_key = \"evaluation_strategy\" if \"evaluation_strategy\" in supported else \"eval_strategy\" if \"eval_strategy\" in supported else None\n",
    "    if eval_key:\n",
    "        filtered_kwargs[eval_key] = save_strategy\n",
    "    else:\n",
    "        filtered_kwargs.pop(\"load_best_model_at_end\", None)\n",
    "        print(\"‚ö†Ô∏è load_best_model_at_end disabled because eval/save strategy key not supported in this transformers version\")\n",
    "\n",
    "training_args = TrainingArguments(**filtered_kwargs)\n",
    "\n",
    "print(f\"‚úÖ Training arguments configured\")\n",
    "print(f\"   Output directory: {output_dir}\")\n",
    "if hasattr(training_args, \"save_steps\"):\n",
    "    print(f\"   Checkpoints will be saved every {training_args.save_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n",
      "   Total training steps: 1821\n"
     ]
    }
   ],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fine-tuning...\n",
      "   Model will be saved to: /Users/rorosaga/Documents/ie_university/year_4/advanced_ai/repos/LOL-LM/data/model/gpt2-satirical-news\n",
      "   Checkpoints will be saved every 500 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='1824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  69/1824 03:00 < 1:18:44, 0.37 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "print(f\"   Model will be saved to: {output_dir}\")\n",
    "print(f\"   Checkpoints will be saved every {training_args.save_steps} steps\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and tokenizer\n",
    "final_model_dir = output_dir / \"final\"\n",
    "final_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(final_model_dir))\n",
    "tokenizer.save_pretrained(str(final_model_dir))\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_model_dir}\")\n",
    "print(f\"   Model files:\")\n",
    "for file in final_model_dir.glob(\"*\"):\n",
    "    print(f\"     - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"Testing the fine-tuned model...\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "test_model = GPT2LMHeadModel.from_pretrained(str(final_model_dir)).to(device)\n",
    "test_tokenizer = GPT2Tokenizer.from_pretrained(str(final_model_dir))\n",
    "\n",
    "# Test generation\n",
    "test_topics = \"politics, election, candidate\"\n",
    "test_prompt = f\"<system>{SYSTEM_PROMPT}<user>Generate an article on: {test_topics}<assistant>\"\n",
    "\n",
    "inputs = test_tokenizer.encode(test_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = test_model.generate(\n",
    "        inputs,\n",
    "        max_length=300,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        pad_token_id=test_tokenizer.pad_token_id,\n",
    "        eos_token_id=test_tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generated_text = test_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_checkpoints",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved checkpoints\n",
    "print(\"üìÅ Saved checkpoints:\")\n",
    "checkpoint_dirs = sorted([d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint')])\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    print(f\"   {checkpoint_dir.name}\")\n",
    "    # Show checkpoint size\n",
    "    total_size = sum(f.stat().st_size for f in checkpoint_dir.rglob('*') if f.is_file())\n",
    "    print(f\"     Size: {total_size / (1024**2):.2f} MB\")\n",
    "\n",
    "if final_model_dir.exists():\n",
    "    final_size = sum(f.stat().st_size for f in final_model_dir.rglob('*') if f.is_file())\n",
    "    print(f\"\\n   Final model: {final_size / (1024**2):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
